/*
 * Copyright 2022 Computer Systems Department, Jozef Stefan Insitute
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    https://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
syntax = "proto3";

package lbir;

option java_multiple_files = true;
option java_package = "lbir";

message Model {
    string name = 1;
    repeated Layer layers = 3;
}

message Layer {
    enum Type {
        DENSE  = 0;
        CONV2D = 1;
        PREPROC = 2;
    }
    enum Activation {
        BINARY_SIGN = 0;
        RELU = 1;
        NO_ACTIVATION = 2;
    }

    PreprocessLayer preprocess_layer = 1;
    Type ltype = 2; // type is a python keyword
    QTensor thresh  = 3;
    QTensor weights = 4;
    QTensor input = 5;  // no values, just shape / dtype
    QTensor output = 6;
    Activation activation = 7;
}

message QTensor {
    Datatype dtype = 1;
    /* KERNEL, CH, HEIGHT, WIDTH (memory layout)
       Shape of QTensor of convolutional kernels is KCHW. Shape of conv2d thresh is (K).
       Shape has no information on batches. So input to conv2d layers are 3d (C,H,W).
       QTensors for dense layers (inputs) should have shape: (n), where n is the
       number of inputs to the layer. A QTensor of the weights in dense layers is of
       shape (m, n). Here n is the number of inputs to a dense layer and m the number
       of outputs. The memory layout of the tensor follows the shape dimensions.
       Note that tensorflow uses the NHWC layout, as opposed to the CHW by LBIR.
    */
    repeated uint32 shape = 2;
    /* A single sequence (Seq[Float]) of the actual values of a given QTensor.
       Actual values are those that are computed in a float implementation. For
       example: binarized neural networks the actual values are (+1,-1), while
       implementation values can be different (1,0).
    */
    repeated float values = 3;
}

message Datatype {
    enum QuantizationType {
        UNIFORM = 0;
        BINARY = 1;
    }
    QuantizationType quantization = 1;
    bool signed = 2;
    uint32 bitwidth = 3;
    /*
       In convolutional layers repeated shift(scale) factors signifies a per-channel quantization. In dense layers it
       is per neuron.
    */
    repeated sint32 shift = 4;
    repeated sint32 offset = 5;
}

message PreprocessLayer {
    enum Type {
        MFCC = 0;
    }
    Type ptype = 1;
    uint32 fft_size = 2;
    uint32 step_size = 3;
    uint32 num_mels = 4;
    uint32 num_frames= 5;
}
